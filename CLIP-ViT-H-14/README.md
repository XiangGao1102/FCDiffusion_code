Our method uses the pretrained OpenCLIP text encoder, download the **open_clip_pytorch_model.bin** file [here](https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K/tree/main) and put it in the **CLIP-ViT-H-14** folder of the project, as shown below:
<div style="padding-left: 4%; padding-right: 4%;">
                <div align="center">
                    <img src="../img/open_clip_model.png" width="70%"> <br>
		</div>
</div>
